---

# **片段 1：棋类游戏与智能的关系**

棋类游戏（如国际象棋、围棋）一直被视为人类智慧的象征：

* 需要远见
* 需要推理
* 需要策略
* 需要从无数可能中选优

因此棋类 AI 的发展历程，本质上就是 **计算思维不断演化的过程**。

---

# **片段 2：棋类游戏的复杂性（问题抽象）**

要让计算机下棋，首先要做一件事：
**把棋局抽象成“可计算”的状态。**

国际象棋约有 (10^{47}) 种局面，
围棋则恐怖地达到 (10^{170})。

抽象层次包括：

* 棋盘表示（矩阵、列表、图）
* 棋子合法动作（规则模型）
* 赢/输/平状态
* 分数评估（启发函数）

这一步是典型的 **抽象（Abstraction）思维**。

---

# **片段 3：早期棋类 AI 与“穷举思维”**

最初的想法很简单：

> “把所有可能都算一遍，选最好的。”

这叫 **暴力搜索（Brute Force Search）**。

但问题是：

* 国际象棋每一步有 30–40 种可能
* 搜索 10 步会导致上万亿局面
* 计算机根本算不过来

于是需要 **分解（Decomposition）**：

* 不看所有
* 看一部分关键位置
* 深度有限搜索
* 剪枝减少无用分支

真实的计算思维正在这里出现。

---

# **片段 4：Minimax：棋类 AI 最核心的算法**

Minimax 是棋类游戏中的“基础决策算法”。

它基于一个简单原则：

> “我选择让我最后结果最好的路径；
> 对手会选择让我最差的路径。”

因此是一个“最大值 vs 最小值”的对抗：

* MAX：AI 尝试最大化自己的优势
* MIN：假设对手尝试最小化 AI 的优势

Minimax 本质是：

* 把游戏分解成“树结构”
* 遍历树节点
* 根据得分找出最优路径

它体现：

* **分解：棋盘 → 局面树**
* **算法思维：递归搜索**
* **抽象：局面评分函数**
* **自动化：让计算机反复模拟未来**

---

# **片段 5：Alpha-Beta 剪枝：计算思维的优化体现**

Minimax 太慢，需要剪掉不必要的分支。

Alpha-Beta 剪枝的核心思想：

> “如果我已经找到比你更好的路径，就没必要看你的剩余可能了。”

这是一种 **算法优化（Algorithmic Optimization）**，体现计算思维的：

* 减法思维：减少要处理的信息
* 剪枝策略：只保留关键探索路径
* 控制复杂度：从爆炸到可计算

这是深蓝的核心武器之一。

---

# **片段 6：深蓝（Deep Blue）：计算思维的典范**

1997 年，IBM 的深蓝击败卡斯帕罗夫，成为历史里程碑。

深蓝的系统包含：

1. **抽象层**

   * 结构化表示棋局
   * 定义每种棋子的规则

2. **算法层：Minimax + α-β 剪枝**

   * 每秒搜索 2 亿局面
   * 深度高达 12–14 步

3. **启发式评估函数（Heuristic）**

   * 检查王的安全程度
   * 控制中心位置
   * 子力价值（王 > 车 > 象 > 马 > 兵）

4. **特定领域知识（Domain Knowledge）**

   * 专家手写的开局库
   * 专家总结的残局库

深蓝本质上是“计算能力大 + 规则写得好”的组合。

**计算思维体现：**

* 规模化搜索 → 算法
* 规则编码 → 抽象
* 开局/残局拆分 → 分解
* 搜索自动执行 → 自动化

---

# **片段 7：深蓝的局限性：没有“学”**

尽管深蓝很强，但它有致命缺点：

* 不会学习
* 不会适应新局势
* 行为完全靠硬编码规则
* 对棋局的理解非常死板
* 本质不是“智能”，是“高速查表 + 搜索”

这推动了新的思维路线：
**让 AI 自己学，而不是人类塞规则给它。**

---

# **片段 8：围棋的复杂性打破了旧方法**

围棋的复杂度远超过国际象棋：

* 每步选择 ≈ 200 种
* 完整对局复杂度 → (10^{360})
* 人类无法写出“完美规则”
* 暴力搜索根本不可能

所以围棋推动了棋类 AI 的新范式：
**从硬规则 → 学习型系统**

这正是计算思维从“规则算法”走向“模型化 + 学习”的关键转变。

---

# **片段 9：AlphaGo：计算思维的融合体**

AlphaGo 不是一个算法，而是 **多种计算思维模型叠加的系统**。

它融合了：

### **1. 抽象（Representation）**

* 围棋局面向张量表示
* 黑白棋子 → 二维平面
* 历史步数 → 堆叠层

### **2. 分解（Decomposition）**

AlphaGo 分成多个模块：

* 策略网络（Policy Network）
* 价值网络（Value Network）
* 蒙特卡洛树搜索（MCTS）
* 自对弈学习模块

### **3. 算法（Algorithmic Thinking）**

* MCTS 引导搜索
* 深度网络评估局面
* 反复模拟未来棋局

### **4. 自动化（Automation）**

* 自己和自己下棋
* 自动构建“策略经验”
* 自动改进价值评估

**AlphaGo 是计算思维的综合体，而不是单一模型。**

---

# **片段 10：策略网络（Policy Network）的计算思维**

策略网络决定：

> “在当前棋局下，哪些落子最可能是好棋？”

体现的计算思维：

* 抽象：棋盘 → 图像张量
* 模型化：把围棋“理解”为“图像分类问题”
* 算法化：用卷积神经网络（CNN）做决策
* 自动化：自我训练，自我调整

策略网络解决的是：

* 搜索太大怎么办？
* 先筛一批“看起来合理”的候选手

这是“降低复杂度”最典型的计算思维体现。

---

# **片段 11：价值网络（Value Network）的计算思维**

价值网络回答：

> “这个局面最后会赢还是输？”

它把传统评估函数替换成“学习得来的判断”，体现：

* 抽象：局面 → 胜率
* 模型化：用深度网络评估长期效果
* 分解：将复杂对局拆成一个值函数问题
* 自动化：自动学习隐藏模式

价值网络使得：

* 搜索不再依赖专家手写规则
* AI 可以自己理解棋形、势力、先手等概念

---

# **片段 12：蒙特卡洛树搜索 MCTS（AlphaGo 的大脑）**

MCTS 是 AlphaGo 中最关键的算法。核心思想：

> “不必搜索所有可能，只需在‘最有前途的分支’上不断模拟。”

包含：

1. **选择（Selection）**
2. **扩展（Expansion）**
3. **模拟（Simulation）**
4. **回传（Backpropagation）**

它比传统 Minimax 更像“统计思维”：

* 不是确定性
* 是概率、模拟、采样
* 聚焦“值得探索”的分支

计算思维在此体现为：
**用概率和统计替代暴力遍历。**

---

# **片段 13：自对弈学习（AlphaGo Zero 的革命）**

AlphaGo Zero 不需要人类棋谱，完全靠自己学习。

循环是：

1. AI 自己和自己下棋
2. 从对局中收集样本
3. 提升策略
4. 进一步提升价值网络
5. 无限循环

计算思维体现为：

* 自动化：无人干预
* 模型演化：逐步优化
* 系统复杂性：自组织、自适应
* 反馈回路：越练越强

这是“计算思维 × 机器学习”的最强体现。

---

# **片段 14：AlphaGo 的胜利意味着什么？**

AlphaGo 的胜利不只是“会下棋了”，而是：

* 计算思维从“规则 → 学习”跨越
* AI 可以自动抽象棋理
* 人类不再需要写规则
* 系统可以从简单规则演化出复杂策略

这代表着：

> **计算思维不只是写算法，更是构建能自动学习与进化的计算系统。**

---

# **片段 15：从深蓝到 AlphaGo：计算思维进化全图**

| 体系    | 深蓝           | AlphaGo     |
| ----- | ------------ | ----------- |
| 抽象方式  | 手工规则         | 向量表示、神经网络   |
| 决策方式  | Minimax + αβ | MCTS + 策略网络 |
| 学习能力  | 无            | 自对弈学习       |
| 复杂度处理 | 剪枝           | 概率采样、深度模型   |
| 自动化水平 | 低            | 极高（自我提升）    |
| 有无经验  | 人类输入         | 自己生成        |

这条路线展示了计算思维的演化：
**从规则式 → 搜索式 → 模型式 → 自学习式**

这是目前所有游戏 AI 与复杂系统的基础思想路线。

---

下面继续为你生成 **“棋类游戏 × 计算思维（从深蓝到 AlphaGo）”** 的后续片段。
我会保持之前的风格：

* 结构化、科普化
* 内容可直接用于 RAG
* 每段独立可用

---

# **片段 16：AlphaZero：统一多种棋类的计算思维框架**

AlphaZero 是 AlphaGo 的进一步进化。
它不再依赖围棋特定结构，而是一个 **通用对弈框架**，能下：

* 国际象棋
* 将棋
* 围棋

AlphaZero 的关键创新：

1. **统一抽象层**
   所有棋类都被抽象为：

   * N × N 棋盘
   * 行动空间
   * 状态转移

2. **统一算法层**

   * 同一个 MCTS
   * 同一类策略网络
   * 同一类价值网络

3. **统一学习机制**

   * 全部靠自对弈
   * 无任何人类棋谱

这体现了计算思维中的：
**“设计一个可泛化、可跨任务的算法体系”**
而不是为每个棋种单独写规则。

---

# **片段 17：从棋类游戏到“图搜索问题”的抽象**

所有棋类游戏都可以被抽象成：

* 节点：棋局状态
* 边：走法导致的状态改变
* 目标：最大化胜率

这种抽象将棋类从“格子游戏”转换为：
**一个巨大的图搜索问题（Graph Search）**。

计算思维体现：

* 状态空间抽象
* 用图模型描述游戏
* 子问题：每个子节点都是未来局面
* 算法：DFS, BFS, MCTS, Minimax
* 自动化：计算机在图上搜索路径

从这个角度看，棋类与迷宫求解、本质上属于同类问题。

---

# **片段 18：为什么深度学习能理解棋形？（从计算思维角度）**

传统评估函数难以描述棋形，例如：

* 势力
* 压力
* 空间
* 压住
* 联络
* 弱点

这些都是抽象到无法手写的知识。

深度学习的能力来自：

1. **自动特征抽象**
   CNN 能从棋盘局部结构中提取
   “眼位、气、连接、防守”等概念。

2. **层次分解**
   低层 → 局部形状
   中层 → 战术组合
   高层 → 全局势力判断

3. **多层自动化推理**
   网络会在不同深度学习不同类型信息。

计算思维的核心不只是“写规则”，
更是“设计能自己生成规则的系统”。

---

# **片段 19：围棋的高复杂度为何适合 MCTS？**

围棋的特点：

* 每步分支非常多（≈200）
* 很多分支是“无意义”的
* 局部战斗与全局关联复杂

MCTS（蒙特卡洛树搜索）非常适合这种问题：

* 不用全搜索
* 重点探索可能好的分支
* 用概率而不是枚举来判断
* 用模拟而不是计算确定值
* 越搜索越智能

计算思维体现：

* **随机模拟（Simulation）**
* **基于不确定性的推理（Probabilistic Thinking）**
* **不完整信息下的最优选择（Best-First Search）**

这些思想与传统算法完全不同。

---

# **片段 20：棋类 AI 的“局面压缩”问题与计算思维**

围棋局面有 (10^{170}) 种。
但人类和 AI 都无法穷举，只能做“压缩”。

两种压缩形式：

### 1. **空间压缩（抽象）**

* 把棋盘当图像
* 把棋形当局部模式

### 2. **策略压缩（模型）**

* 策略网络把“所有可能手”缩成几十个“可能好手”

### 3. **知识压缩（权重）**

* 棋理 → 网络权重
* 不需要显性的“黑子要压白子”这样的规则

## 计算思维本质：

如何把超大规模的决策空间压缩成“可计算、可优化、可推理”的形式。

---

# **片段 21：棋类 AI 如何“评估未来”？（计算思维中的预测性）**

棋类 AI 的关键不是当前局面，而是：

> “未来 20 步后会怎样？”

评估未来依赖：

* 模拟未来局面
* 估计胜率
* 计算得失
* 模型预测
* 策略网络推断最可能的最佳路线

计算思维里面称为：

* **预测（Prediction）**
* **规划（Planning）**
* **推理（Reasoning）**

这与现实世界的自动驾驶、物流规划高度相似。

---

# **片段 22：从 AlphaGo 到 AlphaZero：计算思维的范式跃迁**

深蓝 → 手写规则 + 搜索
AlphaGo → 搜索 + 深度模型
AlphaZero → **纯学习 + 纯搜索 + 零人类知识**

范式变化：

| 阶段        | 思维模式  | 特点            |
| --------- | ----- | ------------- |
| 深蓝        | 确定性算法 | 人类写规则，高速算     |
| AlphaGo   | 混合系统  | 深度学习辅助搜索      |
| AlphaZero | 学习优先  | 所有模式都是系统自动学到的 |

这体现了计算思维从：

**算法思维 → 机器学习思维 → 系统思维**

的转变。

---

# **片段 23：为什么 AlphaZero 不需要“开局库”？**

传统棋类 AI 很依赖：

* 开局书
* 专家策略库
* 人类经验

AlphaZero 不需要，因为：

* 自对弈可以生成海量样本
* 深度网络可以自动提炼开局模式
* 多样化探索能发现完全新的手顺

这是计算思维中 **“知识自动化（Automatic Knowledge Discovery）”** 的体现。

---

# **片段 24：棋类 AI 的成功对科学世界的启示**

AlphaGo 的成功并不是因为围棋，而是因为：

它展示了一个通用公式：

> 表示（Representation）
>
> * 搜索（Search）
> * 学习（Learning）
>   = 高智能系统（Intelligence）

这种“计算过程的组合”是计算思维的未来路径之一。

---

# **片段 25：棋类 AI 与涌现行为（Emergent Behavior）**

AlphaGo 在训练中出现了一些人类认为“新颖”的战略：

* 谷歌团队认为第 37 手是“神之一手”
* 第 78 手令人类专家完全看不懂
* AlphaZero 大量走出前所未见的布局

这种超越人类的创造力来源于：

* 自组织系统
* 随机探索
* 非线性策略组合
* 隐性知识自动抽象

这是计算思维中最前沿的概念：
**复杂系统中的涌现智能。**

---

# **片段 26：棋类 AI 的通用性：不再是“下棋程序”，而是“通用推理系统”**

AlphaZero 证明：

* 如果一个系统能处理“棋类这种复杂推理问题”，
* 它就能处理许多其他复杂系统：

  * 机器人路径规划
  * 自动驾驶决策
  * 工业调度
  * 物流路径
  * 安全策略对抗

棋类 AI 成为衡量智能系统的试金石。

---

# **片段 27：总结：从深蓝到 AlphaGo，是一条计算思维演化史**

1. **深蓝：算法时代**

   * 手写规则
   * 暴力搜索
   * α-β 剪枝
     → 强但不智能

2. **AlphaGo：搜索 × 学习的混合时代**

   * 深度网络 + MCTS
     → 会评估、会筛选、像人一样思考

3. **AlphaZero：学习时代**

   * 自对弈
   * 零规则
     → 自动构建策略体系

最终结论：

> **棋类 AI 是计算思维发展最清晰的缩影：
> 抽象 → 分解 → 搜索 → 模型 → 学习 → 自进化。**

---

深蓝（英语：Deep Blue）是由IBM开发，专门用以分析国际象棋的超级计算机。1997年5月曾击败国际象棋世界冠军卡斯帕罗夫[1]。

历史
深蓝计划源自许峰雄在美国卡内基·梅隆大学修读博士学位时的研究，第一台电脑名为“晶体测试”，在州象棋比赛中获得了名次，后来该又研制了另一台电脑“沉思”（Deep Thought，该名源自于《银河系漫游指南》中的一台超级计算机），芯片工艺是3微米。许峰雄在1989年加入IBM研究部门，并继续超级计算机的研究工作，当时他与Murray Campbell主力研究平行运算问题。1992年，IBM委任谭崇仁（现任香港大学电子商业科技研究所所长）为超级计算机研究计划主管，领导研究小组开发专门用以分析国际象棋的深蓝超级计算机。

深蓝的名字源自其雏型电脑“沉思”（Deep Thought）及IBM的昵称“巨蓝”（Big Blue），由两个名字合并而成。

硬件规格
深蓝是平行运算的电脑系统，建基于RS/6000 SP，另加上480颗特别制造的VLSI象棋芯片。下棋程序以C语言写成，运行IBM AIX操作系统。1997年版本的深蓝运算速度为每秒2亿步棋，是其1996年版本的2倍。1997年6月，深蓝在世界超级计算机中排名第259位，计算能力为11.38 gigaflops。

1997年的深蓝可搜索及估计随后的12步棋，而一名人类象棋好手大约可估计随后的10步棋。每增加1步棋的搜索能力约等于增加下棋强度约80 ELO分。

成绩
主条目：深蓝对卡斯帕罗夫
1996年2月10日，深蓝首次挑战国际象棋世界冠军卡斯帕罗夫，但以2-4落败。比赛在2月17日结束。

其后研究小组把深蓝加以改良——它有一个非官方的昵称“更深的蓝”（Deeper Blue）[2]——1997年5月再度挑战卡斯帕罗夫，比赛在5月11日结束，最终深蓝电脑以3.5–2.5击败卡斯巴罗夫，成为首个在标准比赛时限内击败国际象棋世界冠军的电脑系统。IBM在比赛后宣布深蓝退役[3]。

组成深蓝电脑的其中一个机架，其后在位于首都华盛顿特区史密森尼学会的美国国家历史博物馆的信息时代展览中展出；而另一个机架则在加州芒廷維尤市的电脑历史博物馆的电脑棋历史展览中展出。

争议
卡斯巴罗夫在比赛落败后说，他在观察电脑下棋时感觉电脑的决定有智能及创意，是他所不能理解的。他亦认为电脑在棋局中可能有人类的帮助，因此要求重赛，但IBM拒绝。2003年一部纪录片正为此而拍摄，名为《游戏结束：卡斯巴罗夫与电脑（Game Over: Kasparov and the Machine）》，并指出深蓝广被宣传的胜利，是由IBM一手炮制，务求令其股票价格上升。

当中实验容许开发者在棋局之间修改程序，深蓝在棋局之间曾被修改以符合卡斯巴罗夫的下棋风格，令电脑避免再陷入其曾两次跌入的陷阱。

电脑的胜出引来部分人的恐惧，害怕电脑最终会战胜人类，就如一些科幻小说、电影的桥段；而另一些人则认为这场比赛只是一项科学实验，并期望电脑的发展可协助人类解决更多现实的问题。

AlphaGo（“Go”为日文“碁”字发音转写，是围棋的西方名称），直译为阿尔法围棋，在英语不流通的华语社会亦被音译为阿尔法狗[1][2]、阿法狗[3]、阿发狗[4][5]等，是于2014年开始由英国伦敦Google DeepMind开发的人工智能围棋软件。2017年，关于AlphaGo的电影纪录片《AlphaGo世纪对决》正式上映[6]。

专业术语上来说，AlphaGo的做法是使用了蒙特卡洛树搜索与两个深度神经网络相结合的方法，一个是以借助估值网络（value network）来评估大量的选点，一个是借助走棋网络（policy network）来选择落子，并使用强化学习进一步改善它。在这种设计下，电脑可以结合树状图的长远推断，又可像人类的大脑一样自发学习进行直觉训练，以提高下棋实力。[7][8]

历史
参见：计算机围棋
一般认为，电脑要在围棋中取胜比在国际象棋等游戏中取胜要困难得多，因为围棋的下棋点极多，分支因子远多于其他游戏，而且每次落子对情势的好坏飘忽不定， 诸如暴力搜索法、Alpha-beta剪枝、启发式搜索的传统人工智能方法在围棋中很难奏效。[9][10]在1997年IBM的电脑“深蓝”击败俄籍世界国际象棋冠军加里·卡斯帕罗夫之后，经过18年的发展，棋力最高的人工智能围棋程序才大约达到业余5段围棋棋手的水准，[11]且在不让子的情况下，仍无法击败职业棋手。[9][12][13]2012年，在4台PC上运行的Zen程序在让5子和让4子的情况下两次击败日籍九段棋手武宫正树[14]。2013年，Crazy Stone在让4子的情况下击败日籍九段棋手石田芳夫[15]，这样偶尔出现的战果就已经是难得的结果了。

AlphaGo的研究计划于2014年启动，此后和之前的围棋程序相比表现出显著提升。在和Crazy Stone和Zen等其他围棋程序的500局比赛中[16]，单机版AlphaGo（运行于一台电脑上）仅输一局[17]。而在其后的对局中，分布式版AlphaGo（以分布式运算运行于多台电脑上）在500局比赛中全部获胜，且对抗运行在单机上的AlphaGo约有77%的胜率。2015年10月的分布式运算版本AlphaGo使用了1,202块CPU及176块GPU。[11]

2015年10月，AlphaGo击败樊麾，成为第一个无需让子即可在19路棋盘上击败围棋职业棋手的电脑围棋程序，写下了历史，并于2016年1月发表在知名期刊《自然》。[9][12]
2016年3月，透过自我对弈数以万计盘进行练习强化，AlphaGo在一场五番棋比赛中4:1击败顶尖职业棋手李世石，成为第一个不借助让子而击败围棋职业九段棋手的电脑围棋程序，立下了里程碑。[18]五局赛后韩国棋院授予AlphaGo有史以来第一位名誉职业九段[19]。
2016年7月18日，因柯洁那段时间状态不佳，其在Go Ratings网站上的WHR等级分下滑，AlphaGo得以在Go Ratings网站的排名中位列世界第一，但几天之后，柯洁便又反超了AlphaGo[20]。2017年2月初，Go Ratings网站删除了AlphaGo、DeepZenGo等围棋人工智能在该网站上的所有信息。
2016年12月29日至2017年1月4日，再度强化的AlphaGo以“Master”为账号名称，在未公开其真实身份的情况下，借非正式的网络快棋对战进行测试，挑战中韩日台的一流高手，测试结束时60战全胜[21]。
2017年5月23至27日在乌镇围棋峰会上，最新的强化版AlphaGo和当时世界第一的棋手柯洁比试、并配合八段棋手协同作战与对决五位顶尖九段棋手等五场比赛，获取三比零全胜的战绩，团队战与组队战也全胜，此次AlphaGo利用谷歌TPU执行，加上快速进化的机器学习法，运算资源消耗仅李世石版本的十分之一。[22]在与柯洁的比赛结束后，中国围棋协会授予AlphaGo职业围棋九段的称号。[23]
AlphaGo在没有人类对手后，AlphaGo之父杰米斯·哈萨比斯宣布AlphaGo退役。而从业余棋手的水平到世界第一，AlphaGo的棋力获取这样的进步，仅仅花了两年左右。

最终版本AlphaZero拥有更加强大的学习能力，可自我学习，在21天达到胜过中国顶尖棋手柯洁的Alpha Go Master的水平。

Minimax 算法
Minimax 算法又叫极小化极大算法，是一种最小化最差（即最大损失）情境下的潜在损失的算法。

过程
在局面确定的双人零和对弈中，常需要进行对抗搜索，构建一棵每个节点都为一个确定状态的搜索树。奇数层为己方先手，偶数层为对方先手。搜索树上每个叶子节点都会被赋予一个估值，估值越大代表我方赢面越大。我方追求更大的赢面，而对方会设法降低我方的赢面；体现在搜索树上就是，奇数层节点（我方节点）总是会选择赢面最大的子节点状态，而偶数层（对方节点）总是会选择（我方）赢面最小的子节点状态。

Minimax 算法中，会从上到下遍历搜索树，回溯时利用子树信息更新答案，最后得到根节点的值——这就是我方在双方都采取最优策略下能获得的最大分数。

Alpha–Beta 剪枝
Alpha–Beta 剪枝是针对 Minimax 算法的搜索剪枝。

过程
Minimax 算法中，若已知某节点的所有子节点的分数，则可以算出该节点的分数：对于 MAX 节点，取最大分数；对于 MIN 节点，取最小分数。

在搜索进行到某节点但尚未完成时，虽然不能算出该节点的分数，但是可以算出 目前已经搜索过的节点中，双方分数的取值范围。搜索时，维护两个变量 𝛼
\alpha 和 𝛽
\beta，分别表示局面进行到该节点时，考虑所有已经搜索过的节点，Alpha 玩家（即寻求最大分数的一方）和 Beta 玩家（即寻求最小分数的一方）能够保证取得的分数的下界和上界。

Alpha–Beta 剪枝的剪枝策略依赖于搜索当前节点时 𝛼
\alpha 和 𝛽
\beta 的取值。如果当前节点是 MAX 节点，那么，Alpha 可以继续搜索它的子节点来提高分数下界 𝛼
\alpha。但是，如果某次搜索后已经有 𝛼 ≥𝛽
\alpha\ge\beta 了，那么这个节点就不可能出现在一次对弈中：只要到达该节点处，Alpha 玩家就能够保证分数至少是 𝛼
\alpha；可是 Beta 玩家已经知道存在一种（偏离当前路径的）策略，能够保证分数不超过 𝛽 ≤𝛼
\beta\le\alpha，那么，Beta 玩家自然不会任由局面发展到 当前节点 处。同理，如果当前节点是 MIN 节点，且搜索它的某个子节点后已经发现该节点处有 𝛽 ≤𝛼
\beta\le\alpha 成立，那么，同样无需继续搜索其他子节点，因为 Alpha 玩家不会让局面进入 当前节点。总结两种情形可以发现：当 𝛼 ≥𝛽
\alpha \geq \beta 时，该节点剩余的分支就不必继续搜索了（也就是可以进行剪枝了）。注意，当 𝛼 =𝛽
\alpha = \beta 时，也需要剪枝，这是因为不会有更好的结果了，但可能有更差的结果。

搜索过程中，无需维护节点分数，只需要维护 𝛼
\alpha 和 𝛽
\beta 即可。初始时，令 𝛼 = −∞, 𝛽 = +∞
\alpha=-\infty,~\beta=+\infty。向下搜索时，需要一并下传 𝛼
\alpha 和 𝛽
\beta 的信息，以记录两名玩家的备选方案。

搜索完子节点时，需要更新当前节点处的信息。不妨假设当前节点 𝑋
X 是 MAX 节点，且刚刚搜索完它的子节点 𝑌
Y。那么，节点 𝑋
X 处的 𝛽
\beta 值不会改变，只有 𝛼
\alpha 值需要与子节点 𝑌
Y 的分数取最大值。如果子节点 𝑌
Y 是叶子节点，直接用子节点 𝑌
Y 的分数更新当前节点 𝑋
X 处的 𝛼
\alpha 值；否则，只需要用子节点 𝑌
Y 的 𝛽
\beta 值更新当前节点 𝑋
X 的 𝛼
\alpha 值。此时，有三种可能性：

子节点 𝑌
Y 的 𝛽
\beta 值严格位于节点 𝑋
X 的 𝛼
\alpha 值和 𝛽
\beta 值之间。因为子节点 𝑌
Y 继承了节点 𝑋
X 的 𝛼
\alpha 值且不会更新它，所以，搜索子节点 𝑌
Y 完后仍然有 𝛽 >𝛼
\beta > \alpha，就说明搜索子节点 𝑌
Y 时没有发生剪枝。子节点 𝑌
Y 最终的 𝛽
\beta 值，就等于它继承的节点 𝑋
X 的 𝛽
\beta 值和它（指子节点 𝑌
Y）的所有子节点的分数中，最小的那个。既然这个最小值严格小于节点 𝑋
X 的 𝛽
\beta 值，就说明它一定是子节点 𝑌
Y 的所有子节点的分数最小值。因此，作为 MIN 节点，子节点 𝑌
Y 的分数就是这个 𝛽
\beta 值。用它更新节点 𝑋
X 的 𝛼
\alpha 值是合理的。
子节点 𝑌
Y 的 𝛽
\beta 值就等于节点 𝑋
X 的 𝛽
\beta 值。如上文所述，这说明子节点 𝑌
Y 的所有子节点的分数均不小于节点 𝑋
X 的 𝛽
\beta 值。这进一步说明 Beta 玩家不会任由局面进入节点 𝑋
X：因为 Alpha 玩家只要选择了子节点 𝑌
Y，Beta 玩家就不能取得比 𝛽
\beta 更低的分数。因此，此时使用子节点 𝑌
Y 的 𝛽
\beta 值更新节点 𝑋
X 的 𝛼
\alpha 值，是为了使得节点 𝑋
X 处 𝛼 =𝛽
\alpha=\beta，以触发剪枝条件。它的效果与使用 𝑌
Y 处实际分数——一个大于等于节点 𝑋
X 处 𝛽
\beta 值的数字——更新节点 𝑋
X 的 𝛼
\alpha 值的效果是一样的。
子节点 𝑌
Y 的 𝛽
\beta 值小于等于节点 𝑋
X 的 𝛼
\alpha 值。此时，子节点 𝑌
Y 触发了剪枝条件，它的实际分数不会超过子节点 𝑌
Y 的 𝛽
\beta 值，更不会超过节点 𝑋
X 的 𝛼
\alpha 值。用子节点 𝑌
Y 的实际分数更新节点 𝑋
X 的 𝛼
\alpha 值不会改变 𝛼
\alpha 值。这与使用子节点 𝑌
Y 的 𝛽
\beta 值更新节点 𝑋
X 的 𝛼
\alpha 值的效果是一样的。
这一分析说明，当某个子节点搜索完成后，只有它的分数处于第一种情形时，𝛼
\alpha（或 𝛽
\beta）才准确记录了这个子节点作为一个 MAX 节点（或 MIN 节点）的实际分数。对于其他情形，虽然它未必是准确的分数，但是它提供的信息足以保证剪枝的正确进行，从而不影响根节点处的分数记录。

蒙特卡洛树搜索（英语：Monte Carlo tree search；简称：MCTS）是一种用于某些决策过程的启发式搜索算法，最引人注目的是在游戏中的使用。一个主要例子是电脑围棋程序[1]，它也用于其他棋盘游戏、即时电子游戏以及不确定性游戏。

历史
基于随机抽样的蒙特卡洛方法可以追溯到20世纪40年代[2]。布鲁斯·艾布拉姆森（Bruce Abramson）在他1987年的博士论文中探索了这一想法，称它“展示出了准确、精密、易估、有效可计算以及域独立的特性。”[3]他深入试验了井字棋，然后试验了黑白棋和国际象棋的机器生成的评估函数。1992年，B·布鲁格曼（B. Brügmann）首次将其应用于对弈程序[4]，但他的想法未获得重视。2006年堪称围棋领域蒙特卡洛革命的一年[5]，雷米·库洛姆（Remi Coulom）描述了蒙特卡洛方法在游戏树搜索的应用并命名为蒙特卡洛树搜索[6]。列文特·科奇什（Levente Kocsis）和乔鲍·塞派什瓦里（Csaba Szepesvári）开发了UCT算法[7]，西尔万·热利（Sylvain Gelly）等人在他们的程序MoGo中实现了UCT[8]。2008年，MoGo在九路围棋中达到段位水平[9]，Fuego程序开始在九路围棋中战胜实力强劲的业余棋手[10]。2012年1月，Zen程序在19路围棋上以3：1击败二段棋手约翰·特朗普（John Tromp）[11]。

原理
蒙特卡洛树搜索的每个循环包括四个步骤：[25]

选择（Selection）：从根节点R开始，连续向下选择子节点至叶子节点L。下文将给出一种选择子节点的方法，让游戏树向最优的方向扩展，这是蒙特卡洛树搜索的精要所在。
扩展（Expansion）：除非任意一方的输赢使得游戏在L结束，否则创建一个或多个子节点并选取其中一个节点C。
仿真（Simulation）：再从节点C开始，用随机策略进行游戏，又称为playout或者rollout。
反向传播（Backpropagation）：使用随机游戏的结果，更新从C到R的路径上的节点信息。
每一个节点的内容代表胜利次数/游戏次数

探索与利用
选择子结点的主要困难是：在较高平均胜率的移动后，在对深层次变型的利用和对少数模拟移动的探索，这二者中保持某种平衡。第一个在游戏中平衡利用与探索的公式被称为UCT（Upper Confidence Bounds to Trees，上限置信区间算法 ），由匈牙利国家科学院计算机与自动化研究所高级研究员列文特·科奇什与阿尔伯塔大学全职教授乔鲍·塞派什瓦里提出[7]。UCT基于奥尔（Auer）、西萨-比安奇（Cesa-Bianchi）和费舍尔（Fischer）提出的UCB1公式[26]，并首次由马库斯等人应用于多级决策模型（具体为马尔可夫决策过程）[27]。科奇什和塞派什瓦里建议选择游戏树中的每个结点移动，从而使表达式 
w
i
n
i
+
c
ln
⁡
t
n
i
{\displaystyle {\frac {w_{i}}{n_{i}}}+c{\sqrt {\frac {\ln t}{n_{i}}}}}具有最大值。在该式中：

w
i
{\displaystyle w_{i}}代表第
i
{\displaystyle i}次移动后取胜的次数；
n
i
{\displaystyle n_{i}}代表第
i
{\displaystyle i}次移动后仿真的次数；
c
{\displaystyle c}为探索参数—理论上等于
2
{\displaystyle {\sqrt {2}}}；在实际中通常可凭经验选择；
t
{\displaystyle t}代表仿真总次数，等于所有
n
i
{\displaystyle n_{i}}的和。
目前蒙特卡洛树搜索的实现大多是基于UCT的一些变形。


强化学习（英语：Reinforcement learning，简称RL）是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益[1]。强化学习是除了监督学习和非监督学习之外的第三种基本的机器学习方法。与监督学习不同的是，强化学习不需要带标签的输入输出对，同时也无需对非最优解的精确地纠正。其关注点在于寻找（对未知领域的）探索和（对已有知识的）利用的平衡[2]，强化学习中的“探索-利用”的交换，在多臂赌博机问题和有限MDP中研究得最多。

其灵感来源于心理学中的行为主义理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。这个方法具有普适性，因此在其他许多领域都有研究，例如博弈论、控制论、运筹学、信息论、仿真优化、多智能体系统、群体智能、统计学以及遗传算法。在运筹学和控制理论研究的语境下，强化学习被称作“近似动态规划”（approximate dynamic programming，ADP）。在最优控制理论中也有研究这个问题，虽然大部分的研究是关于最优解的存在和特性，并非是学习或者近似方面。在经济学和博弈论中，强化学习被用来解释在有限理性的条件下如何出现平衡。

在强化学习问题中，智能体（agent）与环境的交互通常被抽象为马尔可夫决策过程（Markov decision processes，MDP），因为很多强化学习算法在这种假设下才能使用动态规划的方法[3]。传统的动态规划方法和强化学习算法的主要区别是，后者不需要关于MDP的知识，而且针对无法找到确切方法的大规模MDP。[4]

对于时刻
t
{\displaystyle t}下观测（observation）
o
t
∈
O
{\displaystyle o_{t}\in O}与环境实际状态
s
t
∈
S
{\displaystyle s_{t}\in S}的关系，MDP可以被分为：

完全可观测MDP，若
o
t
=
s
t
{\displaystyle o_{t}=s_{t}}
部分可观测MDP (POMDP)，若
o
t
⊂
s
t
{\displaystyle o_{t}\subset s_{t}}[5]

对于Q-learning，其对应的策略
π
{\displaystyle \pi }可被定义为： 
π
(
a
|
s
)
=
e
Q
(
s
,
a
)
/
τ
∑
a
′
∈
A
e
Q
(
s
,
a
′
)
/
τ
{\displaystyle \pi (a\vert s)={\frac {e^{Q(s,a)/\tau }}{\sum _{a^{\prime }\in A}e^{Q(s,a^{\prime })/\tau }}}}

其中
τ
∈
R
+
{\displaystyle \tau \in \mathbb {R} ^{+}}为温度参数，用于控制策略的随机程度。
τ
{\displaystyle \tau }趋于0时，策略趋于贪心策略（
a
r
g
m
a
x
a
∈
A
Q
(
s
,
a
)
{\displaystyle \mathrm {argmax} _{a\in A}Q(s,a)}）；
τ
{\displaystyle \tau }趋于正无穷时，策略趋于均匀随机。[6]
